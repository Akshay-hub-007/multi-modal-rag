{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab0f0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bc16e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "352a2f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea622c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_docuents(file_path:str):\n",
    "\n",
    "\n",
    "    elements=partition_pdf(\n",
    "        filename=file_path,\n",
    "        strategy=\"hi_res\",\n",
    "        infer_table_structure=True,\n",
    "        extract_image_block_types=[\"Images\"],\n",
    "        extract_image_block_to_payload=True\n",
    "    )\n",
    "    print(f\"Partition of the {file_path} is completed\")\n",
    "    return elements\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7139813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 3271.69it/s]\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The requested type (Images) doesn't match any available type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition of the attention-is-all-you-need.pdf is completed\n"
     ]
    }
   ],
   "source": [
    "elements=partition_docuents(\"attention-is-all-you-need.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7a4e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a95bc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'NarrativeText',\n",
       " 'element_id': '3ee6b319d76aec309ece8b130d9ce347',\n",
       " 'text': '∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.',\n",
       " 'metadata': {'detection_class_prob': 0.935081958770752,\n",
       "  'coordinates': {'points': ((np.float64(300.0),\n",
       "     np.float64(1658.9668177777778)),\n",
       "    (np.float64(300.0), np.float64(1909.7020622222221)),\n",
       "    (np.float64(1404.3262939453125), np.float64(1909.7020622222221)),\n",
       "    (np.float64(1404.3262939453125), np.float64(1658.9668177777778))),\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1700,\n",
       "   'layout_height': 2200},\n",
       "  'last_modified': '2025-09-07T09:54:26',\n",
       "  'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 1,\n",
       "  'filename': 'attention-is-all-you-need.pdf',\n",
       "  'parent_id': '7799d6166ce1cf2eae47c3ad15853cfe'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[32].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8621ffc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.FigureCaption'>\",\n",
       " \"<class 'unstructured.documents.elements.Footer'>\",\n",
       " \"<class 'unstructured.documents.elements.Formula'>\",\n",
       " \"<class 'unstructured.documents.elements.Header'>\",\n",
       " \"<class 'unstructured.documents.elements.Image'>\",\n",
       " \"<class 'unstructured.documents.elements.ListItem'>\",\n",
       " \"<class 'unstructured.documents.elements.NarrativeText'>\",\n",
       " \"<class 'unstructured.documents.elements.Table'>\",\n",
       " \"<class 'unstructured.documents.elements.Text'>\",\n",
       " \"<class 'unstructured.documents.elements.Title'>\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([str(type(el)) for el in elements])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔨 Creating smart chunks...\n",
      "✅ Created 110 chunks\n"
     ]
    }
   ],
   "source": [
    "def create_chunks_by_title(elements):\n",
    "    \"\"\"Create intelligent chunks using title-based strategy\"\"\"\n",
    "    print(\"🔨 Creating smart chunks...\")\n",
    "    \n",
    "    chunks = chunk_by_title(\n",
    "        elements, # The parsed PDF elements from previous step\n",
    "        new_after_n_chars=2400, # Try to start a new chunk after 2400 characters\n",
    "        combine_text_under_n_chars=500 # Merge tiny chunks under 500 chars with neighbors\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Create chunks\n",
    "chunks = create_chunks_by_title(elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c66f4e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<unstructured.documents.elements.NarrativeText at 0x1d4d415de80>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[11].metadata.orig_elements[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6d8a65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CompositeElement'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunks[11]).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f398d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_content_types(chunk):\n",
    "\n",
    "    content_data={\n",
    "        'text':chunk.text,\n",
    "        'images':[],\n",
    "        'tables':[],\n",
    "        'types':['text']\n",
    "    }\n",
    "\n",
    "    if hasattr(chunk,'metadata') and hasattr(chunk.metadata,'orig_elements'):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "            element_type=type(element).__name__\n",
    "            if element_type==\"Table\":\n",
    "                content_data['types'].append('table')\n",
    "                table_html=getattr(element.metadata,'text_as_html',element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "            elif element_type==\"Image\":\n",
    "                if hasattr(element,'metadata') and hasattr(element.metadata,'image_base64'):\n",
    "                    content_data[\"types\"].append('image')\n",
    "                    content_data[\"types\"].append(element.metadata.image_base64)\n",
    "    content_data['types']=list(set(content_data['types']))\n",
    "\n",
    "    return content_data\n",
    "\n",
    "\n",
    "def create_ai_summary(text:str,tables:list[str],images:list[str]):\n",
    "    \"\"\"crete a  ai summary \"\"\"\n",
    "    try:\n",
    "\n",
    "        llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "        prompt_text = f\"\"\"You are creating a searchable description for document content retrieval.\n",
    "\n",
    "        CONTENT TO ANALYZE:\n",
    "        TEXT CONTENT:\n",
    "        {text}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if tables:\n",
    "            prompt_text += \"TABLES:\\n\"\n",
    "            for i,table in tables:\n",
    "                prompt_text+=table\n",
    "                prompt_text += \"\"\"\n",
    "                YOUR TASK:\n",
    "                Generate a comprehensive, searchable description that covers:\n",
    "\n",
    "                1. Key facts, numbers, and data points from text and tables\n",
    "                2. Main topics and concepts discussed  \n",
    "                3. Questions this content could answer\n",
    "                4. Visual content analysis (charts, diagrams, patterns in images)\n",
    "                5. Alternative search terms users might use\n",
    "\n",
    "                Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "                SEARCHABLE DESCRIPTION:\"\"\"\n",
    "                message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "\n",
    "                for image_base64 in images:\n",
    "                    message_content.append({\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "                    })\n",
    "                message = HumanMessage(content=message_content)\n",
    "                response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ❌ AI summary failed: {e}\")\n",
    "        # Fallback to simple summary\n",
    "        summary = f\"{text[:300]}...\"\n",
    "        if tables:\n",
    "            summary += f\" [Contains {len(tables)} table(s)]\"\n",
    "        if images:\n",
    "            summary += f\" [Contains {len(images)} image(s)]\"\n",
    "        return summary\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8d5455e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Processing chunks with AI Summaries...\n",
      "   Processing chunk 1/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 2/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 3/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 4/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 5/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 6/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 7/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 8/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 9/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 10/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 11/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 12/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 13/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 14/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 15/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 16/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 17/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 18/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 19/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 20/110\n",
      "     Types found: [None, 'image', 'text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 21/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 22/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 23/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 24/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 25/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 26/110\n",
      "     Types found: [None, 'image', 'text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 27/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 28/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 29/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 30/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 31/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 32/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 33/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 34/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 35/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 36/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 37/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 38/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 39/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 40/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 41/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 42/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 43/110\n",
      "     Types found: ['text', 'table']\n",
      "     Tables: 1, Images: 0\n",
      "     → Creating AI summary for mixed content...\n",
      "     ❌ AI summary failed: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "     → AI summary created successfully\n",
      "     → Enhanced content preview: Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 · d) O(1) O(1) Recurrent O(n · d2) O(n) O(n) Convolutional O(k · n · d2) O(1) O(logk(n)) Self-Attention (r...\n",
      "   Processing chunk 44/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 45/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 46/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 47/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 48/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 49/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 50/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 51/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 52/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 53/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 54/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 55/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 56/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 57/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 58/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 59/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 60/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 61/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 62/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 63/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 64/110\n",
      "     Types found: ['text', 'table']\n",
      "     Tables: 1, Images: 0\n",
      "     → Creating AI summary for mixed content...\n",
      "     ❌ AI summary failed: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "     → AI summary created successfully\n",
      "     → Enhanced content preview: Model EN-DE BLEU EN-FR Training EN-DE Cost (FLOPs) EN-FR ByteNet [18] 23.75 Deep-Att + PosUnk 39.2 1.0 - 107° GNMT + RL 8] 24.6 39.92 2.3-10!9 1.4-1070 ConvS28S [9] 25.16 40.46 9.6-10'% 1.5-1070 MoE 2...\n",
      "   Processing chunk 65/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 66/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 67/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 68/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 69/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 70/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 71/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 72/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 73/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 74/110\n",
      "     Types found: ['text', 'table']\n",
      "     Tables: 1, Images: 0\n",
      "     → Creating AI summary for mixed content...\n",
      "     ❌ AI summary failed: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "     → AI summary created successfully\n",
      "     → Enhanced content preview: N dmodel dff h dk dv Pdrop ϵls train steps PPL (dev) BLEU params (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 1 512 512 5.29 24.9 (A) 4 16 128 32 128 32 5.00 4.91 25.5 25.8 32 16 16 5....\n",
      "   Processing chunk 75/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 76/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 77/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 78/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 79/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 80/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 81/110\n",
      "     Types found: ['text', 'table']\n",
      "     Tables: 1, Images: 0\n",
      "     → Creating AI summary for mixed content...\n",
      "     ❌ AI summary failed: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "     → AI summary created successfully\n",
      "     → Enhanced content preview: Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) WSJ only, discriminative 88.3 Petrov et al. (2006) WSJ only, discriminative 90.4 Zhu et al. (2013) (40) WSJ only, discriminative 90.4 Dyer et a...\n",
      "   Processing chunk 82/110\n",
      "     Types found: ['text', 'table']\n",
      "     Tables: 1, Images: 0\n",
      "     → Creating AI summary for mixed content...\n",
      "     ❌ AI summary failed: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "     → AI summary created successfully\n",
      "     → Enhanced content preview: Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) 23] multi-task 93.0 Dyer et al. (2016) generative 93.3... [Contains 1 table(s)]...\n",
      "   Processing chunk 83/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 84/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 85/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 86/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 87/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 88/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 89/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 90/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 91/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 92/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 93/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 94/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 95/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 96/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 97/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 98/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 99/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 100/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 101/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 102/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 103/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 104/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 105/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 106/110\n",
      "     Types found: [None, 'image', 'text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 107/110\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 108/110\n",
      "     Types found: [None, 'image', 'text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 109/110\n",
      "     Types found: [None, 'image', 'text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 110/110\n",
      "     Types found: [None, 'image', 'text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "✅ Processed 110 chunks\n"
     ]
    }
   ],
   "source": [
    "def summarise_chunks(chunks):\n",
    "    \"\"\"Process all chunks with AI Summaries\"\"\"\n",
    "    print(\"🧠 Processing chunks with AI Summaries...\")\n",
    "    \n",
    "    langchain_documents = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk = i + 1\n",
    "        print(f\"   Processing chunk {current_chunk}/{total_chunks}\")\n",
    "        \n",
    "        # Analyze chunk content\n",
    "        content_data = separate_content_types(chunk)\n",
    "        \n",
    "        # Debug prints\n",
    "        print(f\"     Types found: {content_data['types']}\")\n",
    "        print(f\"     Tables: {len(content_data['tables'])}, Images: {len(content_data['images'])}\")\n",
    "        \n",
    "        # Create AI-enhanced summary if chunk has tables/images\n",
    "        if content_data['tables'] or content_data['images']:\n",
    "            print(f\"     → Creating AI summary for mixed content...\")\n",
    "            try:\n",
    "                enhanced_content = create_ai_summary(\n",
    "                    content_data['text'],\n",
    "                    content_data['tables'], \n",
    "                    content_data['images']\n",
    "                )\n",
    "                print(f\"     → AI summary created successfully\")\n",
    "                print(f\"     → Enhanced content preview: {enhanced_content[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"     ❌ AI summary failed: {e}\")\n",
    "                enhanced_content = content_data['text']\n",
    "        else:\n",
    "            print(f\"     → Using raw text (no tables/images)\")\n",
    "            enhanced_content = content_data['text']\n",
    "        \n",
    "        # Create LangChain Document with rich metadata\n",
    "        doc = Document(\n",
    "            page_content=enhanced_content,\n",
    "            metadata={\n",
    "                \"original_content\": json.dumps({\n",
    "                    \"raw_text\": content_data['text'],\n",
    "                    \"tables_html\": content_data['tables'],\n",
    "                    \"images_base64\": content_data['images']\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        langchain_documents.append(doc)\n",
    "    \n",
    "    print(f\"✅ Processed {len(langchain_documents)} chunks\")\n",
    "    return langchain_documents\n",
    "\n",
    "\n",
    "# Process chunks with AI\n",
    "processed_chunks = summarise_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Creating embeddings and storing in ChromaDB...\n",
      "--- Creating vector store ---\n",
      "--- Finished creating vector store ---\n",
      "✅ Vector store created and saved to dbv1/chroma_db\n"
     ]
    }
   ],
   "source": [
    "def create_vector_store(documents, persist_directory=\"dbv1/chroma_db\"):\n",
    "    \"\"\"Create and persist ChromaDB vector store\"\"\"\n",
    "    print(\"🔮 Creating embeddings and storing in ChromaDB...\")\n",
    "        \n",
    "    embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    \n",
    "    # Create ChromaDB vector store\n",
    "    print(\"--- Creating vector store ---\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory, \n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(\"--- Finished creating vector store ---\")\n",
    "    \n",
    "    print(f\"✅ Vector store created and saved to {persist_directory}\")\n",
    "    return vectorstore\n",
    "\n",
    "# Create the vector store\n",
    "db = create_vector_store(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='90bedde1-af9b-4a3e-98cf-c7d54dcd12ca', metadata={'original_content': '{\"raw_text\": \"The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\\\n\\\\n3.1 Encoder and Decoder Stacks\", \"tables_html\": [], \"images_base64\": []}'}, page_content='The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n3.1 Encoder and Decoder Stacks'),\n",
       " Document(id='49051ee7-303d-4232-a1a4-d84b07de9b8e', metadata={'original_content': '{\"raw_text\": \"Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\", \"tables_html\": [], \"images_base64\": []}'}, page_content='Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'),\n",
       " Document(id='77a700e3-49cb-42ad-8a36-5799cca4d864', metadata={'original_content': '{\"raw_text\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\", \"tables_html\": [], \"images_base64\": []}'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the two main components of the Transformer architecture? \"\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(query)\n",
    "\n",
    "# Export to JSON\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
